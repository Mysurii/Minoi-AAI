{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 12:25:21.491088: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-16 12:25:21.491217: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n",
    "from keras.models import model_from_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 41191 characters\n"
     ]
    }
   ],
   "source": [
    "with open(\"hazes.txt\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "    corpus = corpus.lower()\n",
    "    corpus = corpus.replace(',', '')\n",
    "    corpus = corpus.replace('.', '')\n",
    "    corpus = corpus.replace('?', '')\n",
    "    corpus = corpus.replace('`', '')\n",
    "    corpus = corpus.replace('`', '')\n",
    "    corpus = corpus.replace(\"'\", '')\n",
    "    corpus = corpus.replace('\"', '')\n",
    "print(\"Loaded a corpus of {0} characters\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding & Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '(', ')', '1', '2', ':', ';', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Our corpus contains 33 unique characters.\n",
      "{'\\n': 0, ' ': 1, '(': 2, ')': 3, '1': 4, '2': 5, ':': 6, ';': 7, 'a': 8, 'b': 9, 'c': 10, 'd': 11, 'e': 12, 'f': 13, 'g': 14, 'h': 15, 'i': 16, 'j': 17, 'k': 18, 'l': 19, 'm': 20, 'n': 21, 'o': 22, 'p': 23, 'r': 24, 's': 25, 't': 26, 'u': 27, 'v': 28, 'w': 29, 'x': 30, 'y': 31, 'z': 32}\n"
     ]
    }
   ],
   "source": [
    "# Get a unique identifier for each char in the corpus, then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "print(chars)\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# it slices, it dices, it makes julienned datasets!\n",
    "# chop up our data into X and y, slice into roughly (num_chars / skip) overlapping 'sentences'\n",
    "# of length sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41171\n"
     ]
    }
   ],
   "source": [
    "print(len(X_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([21, 1, 32, 22, 22, 21, 1, 29, 8, 25, 1, 14, 16, 25, 26, 12, 24, 12, 21, 1],\n",
       " 17)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[1], y_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 32, 22, 22, 21, 1, 29, 8, 25, 1, 14, 16, 25, 26, 12, 24, 12, 21, 1, 17],\n",
       " 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[2], y_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 41171 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing X and y...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/xm65l11n6pb52jrsdhnd6dtw0000gn/T/ipykernel_9627/2224887287.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.zeros((num_sentences, sentence_length, num_chars), dtype=np.bool)\n",
      "/var/folders/yl/xm65l11n6pb52jrsdhnd6dtw0000gn/T/ipykernel_9627/2224887287.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((num_sentences, num_chars), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=np.bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check y. Dimension: (41171, 33) # Sentences: 41171 Characters in corpus: 33\n",
      "Sanity check X. Dimension: (41171, 20, 33) Sentence length: 20\n"
     ]
    }
   ],
   "source": [
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\".format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\".format(X.shape, sentence_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's build model 1\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 128)               82944     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 33)                4257      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 33)                1122      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 33)                1122      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 33)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,445\n",
      "Trainable params: 89,445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define our model\n",
    "print(\"Let's build model 1\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sentence_length, num_chars)))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 12:25:22.535141: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-16 12:25:22.637355: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-16 12:25:22.696936: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - ETA: 0s - loss: 2.5258\n",
      "Epoch 1: loss improved from inf to 2.52578, saving model to weights-01.hdf5\n",
      "322/322 [==============================] - 6s 15ms/step - loss: 2.5258\n",
      "Epoch 2/20\n",
      "319/322 [============================>.] - ETA: 0s - loss: 2.0239\n",
      "Epoch 2: loss improved from 2.52578 to 2.02400, saving model to weights-02.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 2.0240\n",
      "Epoch 3/20\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.8958\n",
      "Epoch 3: loss improved from 2.02400 to 1.89580, saving model to weights-03.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 1.8958\n",
      "Epoch 4/20\n",
      "321/322 [============================>.] - ETA: 0s - loss: 1.7996\n",
      "Epoch 4: loss improved from 1.89580 to 1.79980, saving model to weights-04.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 1.7998\n",
      "Epoch 5/20\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.7134\n",
      "Epoch 5: loss improved from 1.79980 to 1.71335, saving model to weights-05.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 1.7134\n",
      "Epoch 6/20\n",
      "319/322 [============================>.] - ETA: 0s - loss: 1.6329\n",
      "Epoch 6: loss improved from 1.71335 to 1.63350, saving model to weights-06.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 1.6335\n",
      "Epoch 7/20\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.5560\n",
      "Epoch 7: loss improved from 1.63350 to 1.55597, saving model to weights-07.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 1.5560\n",
      "Epoch 8/20\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.4812\n",
      "Epoch 8: loss improved from 1.55597 to 1.48116, saving model to weights-08.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 1.4812\n",
      "Epoch 9/20\n",
      "319/322 [============================>.] - ETA: 0s - loss: 1.4100\n",
      "Epoch 9: loss improved from 1.48116 to 1.40932, saving model to weights-09.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 1.4093\n",
      "Epoch 10/20\n",
      "320/322 [============================>.] - ETA: 0s - loss: 1.3392\n",
      "Epoch 10: loss improved from 1.40932 to 1.33908, saving model to weights-10.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 1.3391\n",
      "Epoch 11/20\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.2722\n",
      "Epoch 11: loss improved from 1.33908 to 1.27216, saving model to weights-11.hdf5\n",
      "322/322 [==============================] - 5s 16ms/step - loss: 1.2722\n",
      "Epoch 12/20\n",
      "321/322 [============================>.] - ETA: 0s - loss: 1.2025\n",
      "Epoch 12: loss improved from 1.27216 to 1.20260, saving model to weights-12.hdf5\n",
      "322/322 [==============================] - 6s 18ms/step - loss: 1.2026\n",
      "Epoch 13/20\n",
      "321/322 [============================>.] - ETA: 0s - loss: 1.1414\n",
      "Epoch 13: loss improved from 1.20260 to 1.14095, saving model to weights-13.hdf5\n",
      "322/322 [==============================] - 5s 17ms/step - loss: 1.1409\n",
      "Epoch 14/20\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.0773\n",
      "Epoch 14: loss improved from 1.14095 to 1.07729, saving model to weights-14.hdf5\n",
      "322/322 [==============================] - 5s 16ms/step - loss: 1.0773\n",
      "Epoch 15/20\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.0144\n",
      "Epoch 15: loss improved from 1.07729 to 1.01444, saving model to weights-15.hdf5\n",
      "322/322 [==============================] - 5s 16ms/step - loss: 1.0144\n",
      "Epoch 16/20\n",
      "320/322 [============================>.] - ETA: 0s - loss: 0.9571\n",
      "Epoch 16: loss improved from 1.01444 to 0.95753, saving model to weights-16.hdf5\n",
      "322/322 [==============================] - 5s 16ms/step - loss: 0.9575\n",
      "Epoch 17/20\n",
      "320/322 [============================>.] - ETA: 0s - loss: 0.9025\n",
      "Epoch 17: loss improved from 0.95753 to 0.90222, saving model to weights-17.hdf5\n",
      "322/322 [==============================] - 5s 16ms/step - loss: 0.9022\n",
      "Epoch 18/20\n",
      "319/322 [============================>.] - ETA: 0s - loss: 0.8502\n",
      "Epoch 18: loss improved from 0.90222 to 0.85097, saving model to weights-18.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 0.8510\n",
      "Epoch 19/20\n",
      "322/322 [==============================] - ETA: 0s - loss: 0.7966\n",
      "Epoch 19: loss improved from 0.85097 to 0.79662, saving model to weights-19.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 0.7966\n",
      "Epoch 20/20\n",
      "321/322 [============================>.] - ETA: 0s - loss: 0.7512\n",
      "Epoch 20: loss improved from 0.79662 to 0.75126, saving model to weights-20.hdf5\n",
      "322/322 [==============================] - 5s 15ms/step - loss: 0.7513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c2daf520>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump our model architecture to a file so we can load it elsewhere\n",
    "# Find out how to load a model? ,\n",
    "# return_sequences=True\n",
    "architecture = model.to_json()\n",
    "with open('model.json', 'w') as model_file:\n",
    "    model_file.write(architecture)\n",
    "\n",
    "# Set up checkpoints, and save trained model\n",
    "file_path=\"weights-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# Find out how to load the trained checkpoint?\n",
    "# Lets go, action time!\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# model.add(GRU(256),return_sequences=True)\n",
    "# model.add(GRU(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_length = len(corpus)\n",
    "chars = sorted(list(set(corpus)))\n",
    "sentence_length = 20\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seed_pattern):\n",
    "        X = np.zeros((1, sentence_length, num_chars), dtype=np.bool)\n",
    "        #print(X.shape)\n",
    "        for i, character in enumerate(seed_pattern):\n",
    "            X[0, i, encoding[character]] = 1\n",
    "        \n",
    "        generated_text = \"\"\n",
    "        for i in range(500):\n",
    "            # even de temperatuur toevoegen.\n",
    "            prediction = sample(model.predict(X, verbose=0)[0],1.0)\n",
    "            generated_text += decoding[prediction]\n",
    "\n",
    "            activations = np.zeros((1, 1, num_chars), dtype=np.bool)\n",
    "            activations[0, 0, prediction] = 1\n",
    "            X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:\n",
    "            seed = randint(0, corpus_length - sentence_length)\n",
    "            pattern = abba_corpus[seed:seed + sentence_length]\n",
    "\n",
    "        return pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the bard and show\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/xm65l11n6pb52jrsdhnd6dtw0000gn/T/ipykernel_9627/3962742590.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.zeros((1, sentence_length, num_chars), dtype=np.bool)\n",
      "/var/folders/yl/xm65l11n6pb52jrsdhnd6dtw0000gn/T/ipykernel_9627/3962742590.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  activations = np.zeros((1, 1, num_chars), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "ik ben met me me lield\n",
      "dat jij hebt n beteruwngen dan eer ling zo slecht\n",
      "\n",
      "ik lich nu af leten\n",
      "ik voel ma vool de mijsschiek een knes mijn leren staan\n",
      "ik voel daar kom die zet lecht\n",
      "dat ik du nogt gant 1 naar zien want geroeft\n",
      "want verdat ik zoen en mij\n",
      "zot het wert doe kank het straks zo een lacht\n",
      "gekijk maak neer ach de kroes een vriend\n",
      "zo heb ik het nooit gevoeld\n",
      "nee zo had jie de mijs kranter gaan\n",
      "het is toen zo er nurgenter\n",
      "gaat ok heb ben zie ben ik vriegd\n",
      "dat zoven mij gelooven ik heb ze\n"
     ]
    }
   ],
   "source": [
    "seed = make_seed('in the bard and show you on your lovelight')\n",
    "print(seed)\n",
    "txt =  generate(seed)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluatie\n",
    "\n",
    "Ook vandaag vond ik het heel leuk gaan. Leuk om te zien hoe je een neurale netwerk kan bouwen en gewoon een text ermee kan generaten. Echt vet om te zien. Vandaag heb ik dus ook veel geleerd. Wel vind ik het jammer dat veel code al meegegeven was."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
